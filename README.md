# Flowers classification with tpus

## 项目架构

**Analyse文件夹：包含对数据集的分析，以及用不同的网络架构对性能的影响**

**data文件夹：包含网络架构文件，日志文件，以及class_weight参数**

**deprecate文件夹：已经弃用的py文件，多用于测试**

**hyper_opt文件夹：超参数优化过程及结果**

**models文件夹：包含每个架构整个流程的code,performance and output files**

**utilities文件夹：一些对超参数优化有帮助的可视化小脚本**

## 开发过程

### （1）自定义网络ZNN，效果很差，且精度和loss波动很大，这个应该是学习率的问题。

### （2）使用VGG16与ImageNet预训练参数，首先完全训练，后面冻结VGG，找到最好的冰封点（即：从某一层开始可以训练VGG的参数）

### （3）最优化vgg网络的学习率，设计学习率调度函数（简单的在前几轮上调lr，然后从某个epoch开始缓缓下降）

### （4）发现数据集极度不平衡，试图在fit时使用class_weight

### （5）设计两种版本的class_weight

### （6）发现class_weight效果一般，几乎没有提升。又去调参：Dropout率，数据增强方法（加亮度，加饱和度的调整），效果也一般，只能到78%，接近收敛点了。

### （7）怀疑是分类器的过拟合，了解到现代分类器常采用全局平均池化特征图，而不是把特征图的每个特征都进行分析。这样减少了参数，抑制了过拟合。用后确实有提升，反复调节dropout，最后得到82%精度，极限了。

### （8）试图使用其他预训练模型，采用了一些参考书的建议，使用了Xception和ResNet，发现res效果很好，没怎么调参就直接把我之前的VGG秒杀（85%）。而且训练速度快，应该是和他的学习小路有关系；而Xception不行，效果不好，遂弃用。

### （9）与VGG工作类似（2）->（7），得到88%精度，接近收敛点了。

### （10）试图增大训练数据集，发现Kaggle讨论区有人公开了牛津大学的花蕊数据集，并且已经做好数据清洗了，我也跟着采用了，提升约2%，达到90%

### （11）继续Google，看看还有没有什么办法提升精度。发现一篇论文提到，网络的深度，宽度，以及图片的分辨率都能影响到网络性能。通过寻找这3个参数的最优组合，就能得到最大化网络性能。它便是EfficientNet系列。

### （12）我用512x512的分辨率，所以用b7去训练。结果出乎我的预料，居然达到了93.6%（参数还不是最优化的情况下）

### （13）这次我没有对ent进行最优学习率的搜索，而是通过前面的经验，沿用了2e-4这个学习率作为初始学习率（vgg和res的opt_lr都差不多是这个），并设计学习率调度函数（cos版和sigmoid版,对比使用后sigmoid更好些），然后，完全训练，再到冻结部分ENET层，找到最好的冰封点，最后95.4%精度，95.6%的召回率(验证集），测试集94.453%。至此，仅使用官方数据集，达到的最大精度。



### 题外话：	

### 	经过浏览，Kaggle有人不知道怎么找到60000多张训练数据，且里面有少量的测试集数据！！用了之后测试集精度达到了97.55%！！但显然是没有什么意义的。后来，有人又将这60000多张照片剔除了测试图片，我再次训练后，精度也没有变化。不知道Kaggle承不承认这个数据集咯，不过这已经是题外话了。

